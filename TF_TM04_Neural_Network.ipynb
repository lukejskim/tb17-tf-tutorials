{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S04. 기본 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification.py\n",
    "> 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "# 다음과 같은 형식을 one-hot 형식의 데이터라고 합니다.\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 신경망은 2차원으로 [입력층(특성), 출력층(레이블)] -> [2, 3] 으로 정합니다.\n",
    "W = tf.Variable(tf.random_uniform([2, 3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# 편향은 아웃풋의 갯수, 즉 최종 결과값의 분류 갯수인 3으로 설정합니다.\n",
    "b = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망에 가중치 W과 편향 b을 적용합니다\n",
    "L = tf.add(tf.matmul(X, W), b)\n",
    "# 가중치와 편향을 이용해 계산한 결과 값에\n",
    "# 텐서플로우에서 기본적으로 제공하는 활성화 함수인 ReLU 함수를 적용합니다.\n",
    "L = tf.nn.relu(L)\n",
    "\n",
    "# 마지막으로 softmax 함수를 이용하여 출력값을 사용하기 쉽게 만듭니다\n",
    "# softmax 함수는 다음처럼 결과값을 전체합이 1인 확률로 만들어주는 함수입니다.\n",
    "# 예) [8.04, 2.76, -6.52] -> [0.53 0.24 0.23]\n",
    "model = tf.nn.softmax(L)\n",
    "\n",
    "# 신경망을 최적화하기 위한 비용 함수를 작성합니다.\n",
    "# 각 개별 결과에 대한 합을 구한 뒤 평균을 내는 방식을 사용합니다.\n",
    "# 전체 합이 아닌, 개별 결과를 구한 뒤 평균을 내는 방식을 사용하기 위해 axis 옵션을 사용합니다.\n",
    "# axis 옵션이 없으면 -1.09 처럼 총합인 스칼라값으로 출력됩니다.\n",
    "#        Y         model         Y * tf.log(model)   reduce_sum(axis=1)\n",
    "# 예) [[1 0 0]  [[0.1 0.7 0.2]  -> [[-1.0  0    0]  -> [-1.0, -0.09]\n",
    "#     [0 1 0]]  [0.2 0.8 0.0]]     [ 0   -0.09 0]]\n",
    "# 즉, 이것은 예측값과 실제값 사이의 확률 분포의 차이를 비용으로 계산한 것이며,\n",
    "# 이것을 Cross-Entropy 라고 합니다.\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.13607\n",
      "20 1.13232\n",
      "30 1.12865\n",
      "40 1.12506\n",
      "50 1.12155\n",
      "60 1.11812\n",
      "70 1.11475\n",
      "80 1.11146\n",
      "90 1.10824\n",
      "100 1.10507\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [0 1 1 0 0 1]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 66.67\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "# tf.argmax: 예측값과 실제값의 행렬에서 tf.argmax 를 이용해 가장 큰 값을 가져옵니다.\n",
    "# 예) [[0 1 0] [1 0 0]] -> [1 0]\n",
    "#    [[0.2 0.7 0.1] [0.9 0.1 0.]] -> [1 0]\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf4-2_deep_nn.py\n",
    "> 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델\n",
    "- 신경망의 레이어를 여러개로 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 첫번째 가중치의 차원은 [특성, 히든 레이어의 뉴런갯수] -> [2, 10] 으로 정합니다.\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.))\n",
    "# 두번째 가중치의 차원을 [첫번째 히든 레이어의 뉴런 갯수, 분류 갯수] -> [10, 3] 으로 정합니다.\n",
    "W2 = tf.Variable(tf.random_uniform([10, 3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# b1 은 히든 레이어의 뉴런 갯수로, b2 는 최종 결과값 즉, 분류 갯수인 3으로 설정합니다.\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "b2 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망의 히든 레이어에 가중치 W1과 편향 b1을 적용합니다\n",
    "L1 = tf.add(tf.matmul(X, W1), b1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "# 최종적인 아웃풋을 계산합니다.\n",
    "# 히든레이어에 두번째 가중치 W2와 편향 b2를 적용하여 3개의 출력값을 만들어냅니다.\n",
    "model = tf.add(tf.matmul(L1, W2), b2)\n",
    "\n",
    "# 텐서플로우에서 기본적으로 제공되는 크로스 엔트로피 함수를 이용해\n",
    "# 복잡한 수식을 사용하지 않고도 최적화를 위한 비용 함수를 다음처럼 간단하게 적용할 수 있습니다.\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.854946\n",
      "20 0.64305\n",
      "30 0.472579\n",
      "40 0.334084\n",
      "50 0.226767\n",
      "60 0.149632\n",
      "70 0.0988535\n",
      "80 0.067329\n",
      "90 0.0477552\n",
      "100 0.0355056\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [0 1 2 0 0 2]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 100.00\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf4-3_word2vec.py\n",
    "> Word2Vec 모델을 간단하게 구현해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# matplot 에서 한글을 표시하기 위한 설정\n",
    "mac_font = \"/Library/Fonts/NanumGothic.otf\"\n",
    "win_font = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "font_name = matplotlib.font_manager.FontProperties(fname=win_font).get_name()\n",
    "matplotlib.rc('font', family=font_name)\n",
    "\n",
    "# from matplotlib import font_manager, rc\n",
    "# font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "# rc('font', family=font_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 단어 벡터를 분석해볼 임의의 문장들\n",
    "sentences = [\"나 고양이 좋다\",\n",
    "             \"나 강아지 좋다\",\n",
    "             \"나 동물 좋다\",\n",
    "             \"강아지 고양이 동물\",\n",
    "             \"여자친구 고양이 강아지 좋다\",\n",
    "             \"고양이 생선 우유 좋다\",\n",
    "             \"강아지 생선 싫다 우유 좋다\",\n",
    "             \"강아지 고양이 눈 좋다\",\n",
    "             \"나 여자친구 좋다\",\n",
    "             \"여자친구 나 싫다\",\n",
    "             \"여자친구 나 영화 책 음악 좋다\",\n",
    "             \"나 게임 만화 애니 좋다\",\n",
    "             \"고양이 강아지 싫다\",\n",
    "             \"강아지 고양이 좋다\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나',\n",
       " '고양이',\n",
       " '좋다',\n",
       " '나',\n",
       " '강아지',\n",
       " '좋다',\n",
       " '나',\n",
       " '동물',\n",
       " '좋다',\n",
       " '강아지',\n",
       " '고양이',\n",
       " '동물',\n",
       " '여자친구',\n",
       " '고양이',\n",
       " '강아지',\n",
       " '좋다',\n",
       " '고양이',\n",
       " '생선',\n",
       " '우유',\n",
       " '좋다',\n",
       " '강아지',\n",
       " '생선',\n",
       " '싫다',\n",
       " '우유',\n",
       " '좋다',\n",
       " '강아지',\n",
       " '고양이',\n",
       " '눈',\n",
       " '좋다',\n",
       " '나',\n",
       " '여자친구',\n",
       " '좋다',\n",
       " '여자친구',\n",
       " '나',\n",
       " '싫다',\n",
       " '여자친구',\n",
       " '나',\n",
       " '영화',\n",
       " '책',\n",
       " '음악',\n",
       " '좋다',\n",
       " '나',\n",
       " '게임',\n",
       " '만화',\n",
       " '애니',\n",
       " '좋다',\n",
       " '고양이',\n",
       " '강아지',\n",
       " '싫다',\n",
       " '강아지',\n",
       " '고양이',\n",
       " '좋다']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장을 전부 합친 후 공백으로 단어들을 나누고 고유한 단어들로 리스트를 만듭니다.\n",
    "word_sequence = \" \".join(sentences).split()\n",
    "word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나',\n",
       " '좋다',\n",
       " '여자친구',\n",
       " '영화',\n",
       " '음악',\n",
       " '생선',\n",
       " '동물',\n",
       " '책',\n",
       " '만화',\n",
       " '싫다',\n",
       " '게임',\n",
       " '우유',\n",
       " '고양이',\n",
       " '눈',\n",
       " '애니',\n",
       " '강아지']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'강아지': 15,\n",
       " '게임': 10,\n",
       " '고양이': 12,\n",
       " '나': 0,\n",
       " '눈': 13,\n",
       " '동물': 6,\n",
       " '만화': 8,\n",
       " '생선': 5,\n",
       " '싫다': 9,\n",
       " '애니': 14,\n",
       " '여자친구': 2,\n",
       " '영화': 3,\n",
       " '우유': 11,\n",
       " '음악': 4,\n",
       " '좋다': 1,\n",
       " '책': 7}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자열로 분석하는 것 보다, 숫자로 분석하는 것이 훨씬 용이하므로\n",
    "# 리스트에서 문자들의 인덱스를 뽑아서 사용하기 위해,\n",
    "# 이를 표현하기 위한 연관 배열과, 단어 리스트에서 단어를 참조 할 수 있는 인덱스 배열을 만듭합니다.\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 윈도우 사이즈를 1 로 하는 skip-gram 모델을 만듭니다.\n",
    "# 예) 나 게임 만화 애니 좋다\n",
    "#   -> ([나, 만화], 게임), ([게임, 애니], 만화), ([만화, 좋다], 애니)\n",
    "#   -> (게임, 나), (게임, 만화), (만화, 게임), (만화, 애니), (애니, 만화), (애니, 좋다)\n",
    "skip_grams = []\n",
    "\n",
    "for i in range(1, len(word_sequence) - 1):\n",
    "    # (context, target) : ([target index - 1, target index + 1], target)\n",
    "    # 스킵그램을 만든 후, 저장은 단어의 고유 번호(index)로 저장합니다\n",
    "    target = word_dict[word_sequence[i]]\n",
    "    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
    "\n",
    "    # (target, context[0]), (target, context[1])..\n",
    "    for w in context:\n",
    "        skip_grams.append([target, w])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# skip-gram 데이터에서 무작위로 데이터를 뽑아 입력값과 출력값의 배치 데이터를 생성하는 함수\n",
    "def random_batch(data, size):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
    "\n",
    "    for i in random_index:\n",
    "        random_inputs.append(data[i][0])  # target\n",
    "        random_labels.append([data[i][1]])  # context word\n",
    "\n",
    "    return random_inputs, random_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########\n",
    "# 옵션 설정\n",
    "######\n",
    "# 학습을 반복할 횟수\n",
    "training_epoch = 300\n",
    "# 학습률\n",
    "learning_rate = 0.1\n",
    "# 한 번에 학습할 데이터의 크기\n",
    "batch_size = 20\n",
    "# 단어 벡터를 구성할 임베딩 차원의 크기\n",
    "# 이 예제에서는 x, y 그래프로 표현하기 쉽게 2 개의 값만 출력하도록 합니다.\n",
    "embedding_size = 2\n",
    "# word2vec 모델을 학습시키기 위한 nce_loss 함수에서 사용하기 위한 샘플링 크기\n",
    "# batch_size 보다 작아야 합니다.\n",
    "num_sampled = 15\n",
    "# 총 단어 갯수\n",
    "voc_size = len(word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "# tf.nn.nce_loss 를 사용하려면 출력값을 이렇게 [batch_size, 1] 구성해야합니다.\n",
    "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "# word2vec 모델의 결과 값인 임베딩 벡터를 저장할 변수입니다.\n",
    "# 총 단어 갯수와 임베딩 갯수를 크기로 하는 두 개의 차원을 갖습니다.\n",
    "embeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "# 임베딩 벡터의 차원에서 학습할 입력값에 대한 행들을 뽑아옵니다.\n",
    "# 예) embeddings     inputs    selected\n",
    "#    [[1, 2, 3]  -> [2, 3] -> [[2, 3, 4]\n",
    "#     [2, 3, 4]                [3, 4, 5]]\n",
    "#     [3, 4, 5]\n",
    "#     [4, 5, 6]]\n",
    "selected_embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "# nce_loss 함수에서 사용할 변수들을 정의합니다.\n",
    "nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "\n",
    "# nce_loss 함수를 직접 구현하려면 매우 복잡하지만,\n",
    "# 함수를 텐서플로우가 제공하므로 그냥 tf.nn.nce_loss 함수를 사용하기만 하면 됩니다.\n",
    "loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(nce_weights, nce_biases, labels, selected_embed, num_sampled, voc_size))\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step  10 :  4.93481\n",
      "loss at step  20 :  3.71287\n",
      "loss at step  30 :  3.56373\n",
      "loss at step  40 :  3.54774\n",
      "loss at step  50 :  3.48443\n",
      "loss at step  60 :  3.29484\n",
      "loss at step  70 :  2.95324\n",
      "loss at step  80 :  3.44556\n",
      "loss at step  90 :  3.40839\n",
      "loss at step  100 :  3.16072\n",
      "loss at step  110 :  3.41217\n",
      "loss at step  120 :  3.31814\n",
      "loss at step  130 :  3.35614\n",
      "loss at step  140 :  3.09853\n",
      "loss at step  150 :  3.40529\n",
      "loss at step  160 :  2.99052\n",
      "loss at step  170 :  3.1666\n",
      "loss at step  180 :  3.10169\n",
      "loss at step  190 :  2.91225\n",
      "loss at step  200 :  3.15893\n",
      "loss at step  210 :  3.14349\n",
      "loss at step  220 :  3.01521\n",
      "loss at step  230 :  3.31752\n",
      "loss at step  240 :  3.20149\n",
      "loss at step  250 :  2.94446\n",
      "loss at step  260 :  2.91131\n",
      "loss at step  270 :  3.15684\n",
      "loss at step  280 :  3.08649\n",
      "loss at step  290 :  3.24133\n",
      "loss at step  300 :  2.84614\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_epoch + 1):\n",
    "        batch_inputs, batch_labels = random_batch(skip_grams, batch_size)\n",
    "\n",
    "        _, loss_val = sess.run([train_op, loss],\n",
    "                               feed_dict={inputs: batch_inputs,\n",
    "                                          labels: batch_labels})\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(\"loss at step \", step, \": \", loss_val)\n",
    "\n",
    "    # matplot 으로 출력하여 시각적으로 확인해보기 위해\n",
    "    # 임베딩 벡터의 결과 값을 계산하여 저장합니다.\n",
    "    # with 구문 안에서는 sess.run 대신 간단히 eval() 함수를 사용할 수 있습니다.\n",
    "    trained_embeddings = embeddings.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAD7CAYAAACYLnSTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2cbf50a4898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########\n",
    "# 임베딩된 Word2Vec 결과 확인\n",
    "# 결과는 해당 단어들이 얼마나 다른 단어와 인접해 있는지를 보여줍니다.\n",
    "######\n",
    "for i, label in enumerate(word_list):\n",
    "    x, y = trained_embeddings[i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext=(5, 2),\n",
    "                 textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf4-1_classification.py \n",
    "<hr>\n",
    "``` python\n",
    "# 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델을 만들어봅니다.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "# 다음과 같은 형식을 one-hot 형식의 데이터라고 합니다.\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 신경망은 2차원으로 [입력층(특성), 출력층(레이블)] -> [2, 3] 으로 정합니다.\n",
    "W = tf.Variable(tf.random_uniform([2, 3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# 편향은 아웃풋의 갯수, 즉 최종 결과값의 분류 갯수인 3으로 설정합니다.\n",
    "b = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망에 가중치 W과 편향 b을 적용합니다\n",
    "L = tf.add(tf.matmul(X, W), b)\n",
    "# 가중치와 편향을 이용해 계산한 결과 값에\n",
    "# 텐서플로우에서 기본적으로 제공하는 활성화 함수인 ReLU 함수를 적용합니다.\n",
    "L = tf.nn.relu(L)\n",
    "\n",
    "# 마지막으로 softmax 함수를 이용하여 출력값을 사용하기 쉽게 만듭니다\n",
    "# softmax 함수는 다음처럼 결과값을 전체합이 1인 확률로 만들어주는 함수입니다.\n",
    "# 예) [8.04, 2.76, -6.52] -> [0.53 0.24 0.23]\n",
    "model = tf.nn.softmax(L)\n",
    "\n",
    "# 신경망을 최적화하기 위한 비용 함수를 작성합니다.\n",
    "# 각 개별 결과에 대한 합을 구한 뒤 평균을 내는 방식을 사용합니다.\n",
    "# 전체 합이 아닌, 개별 결과를 구한 뒤 평균을 내는 방식을 사용하기 위해 axis 옵션을 사용합니다.\n",
    "# axis 옵션이 없으면 -1.09 처럼 총합인 스칼라값으로 출력됩니다.\n",
    "#        Y         model         Y * tf.log(model)   reduce_sum(axis=1)\n",
    "# 예) [[1 0 0]  [[0.1 0.7 0.2]  -> [[-1.0  0    0]  -> [-1.0, -0.09]\n",
    "#     [0 1 0]]  [0.2 0.8 0.0]]     [ 0   -0.09 0]]\n",
    "# 즉, 이것은 예측값과 실제값 사이의 확률 분포의 차이를 비용으로 계산한 것이며,\n",
    "# 이것을 Cross-Entropy 라고 합니다.\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "\n",
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "# tf.argmax: 예측값과 실제값의 행렬에서 tf.argmax 를 이용해 가장 큰 값을 가져옵니다.\n",
    "# 예) [[0 1 0] [1 0 0]] -> [1 0]\n",
    "#    [[0.2 0.7 0.1] [0.9 0.1 0.]] -> [1 0]\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf4-2_deep_nn.py \n",
    "<hr>\n",
    "``` python\n",
    "# 털과 날개가 있는지 없는지에 따라, 포유류인지 조류인지 분류하는 신경망 모델을 만들어봅니다.\n",
    "# 신경망의 레이어를 여러개로 구성하여 말로만 듣던 딥러닝을 구성해 봅시다!\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],  # 기타\n",
    "    [0, 1, 0],  # 포유류\n",
    "    [0, 0, 1],  # 조류\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 첫번째 가중치의 차원은 [특성, 히든 레이어의 뉴런갯수] -> [2, 10] 으로 정합니다.\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.))\n",
    "# 두번째 가중치의 차원을 [첫번째 히든 레이어의 뉴런 갯수, 분류 갯수] -> [10, 3] 으로 정합니다.\n",
    "W2 = tf.Variable(tf.random_uniform([10, 3], -1., 1.))\n",
    "\n",
    "# 편향을 각각 각 레이어의 아웃풋 갯수로 설정합니다.\n",
    "# b1 은 히든 레이어의 뉴런 갯수로, b2 는 최종 결과값 즉, 분류 갯수인 3으로 설정합니다.\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "b2 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망의 히든 레이어에 가중치 W1과 편향 b1을 적용합니다\n",
    "L1 = tf.add(tf.matmul(X, W1), b1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "# 최종적인 아웃풋을 계산합니다.\n",
    "# 히든레이어에 두번째 가중치 W2와 편향 b2를 적용하여 3개의 출력값을 만들어냅니다.\n",
    "model = tf.add(tf.matmul(L1, W2), b2)\n",
    "\n",
    "# 텐서플로우에서 기본적으로 제공되는 크로스 엔트로피 함수를 이용해\n",
    "# 복잡한 수식을 사용하지 않고도 최적화를 위한 비용 함수를 다음처럼 간단하게 적용할 수 있습니다.\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "\n",
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf3_word2vec.py \n",
    "<hr>\n",
    "``` python\n",
    "# Word2Vec 모델을 간단하게 구현해봅니다.\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# matplot 에서 한글을 표시하기 위한 설정\n",
    "font_name = matplotlib.font_manager.FontProperties(\n",
    "                fname=\"/Library/Fonts/NanumGothic.otf\"  # 한글 폰트 위치를 넣어주세요\n",
    "            ).get_name()\n",
    "matplotlib.rc('font', family=font_name)\n",
    "\n",
    "# 단어 벡터를 분석해볼 임의의 문장들\n",
    "sentences = [\"나 고양이 좋다\",\n",
    "             \"나 강아지 좋다\",\n",
    "             \"나 동물 좋다\",\n",
    "             \"강아지 고양이 동물\",\n",
    "             \"여자친구 고양이 강아지 좋다\",\n",
    "             \"고양이 생선 우유 좋다\",\n",
    "             \"강아지 생선 싫다 우유 좋다\",\n",
    "             \"강아지 고양이 눈 좋다\",\n",
    "             \"나 여자친구 좋다\",\n",
    "             \"여자친구 나 싫다\",\n",
    "             \"여자친구 나 영화 책 음악 좋다\",\n",
    "             \"나 게임 만화 애니 좋다\",\n",
    "             \"고양이 강아지 싫다\",\n",
    "             \"강아지 고양이 좋다\"]\n",
    "\n",
    "# 문장을 전부 합친 후 공백으로 단어들을 나누고 고유한 단어들로 리스트를 만듭니다.\n",
    "word_sequence = \" \".join(sentences).split()\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "# 문자열로 분석하는 것 보다, 숫자로 분석하는 것이 훨씬 용이하므로\n",
    "# 리스트에서 문자들의 인덱스를 뽑아서 사용하기 위해,\n",
    "# 이를 표현하기 위한 연관 배열과, 단어 리스트에서 단어를 참조 할 수 있는 인덱스 배열을 만듭합니다.\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "\n",
    "# 윈도우 사이즈를 1 로 하는 skip-gram 모델을 만듭니다.\n",
    "# 예) 나 게임 만화 애니 좋다\n",
    "#   -> ([나, 만화], 게임), ([게임, 애니], 만화), ([만화, 좋다], 애니)\n",
    "#   -> (게임, 나), (게임, 만화), (만화, 게임), (만화, 애니), (애니, 만화), (애니, 좋다)\n",
    "skip_grams = []\n",
    "\n",
    "for i in range(1, len(word_sequence) - 1):\n",
    "    # (context, target) : ([target index - 1, target index + 1], target)\n",
    "    # 스킵그램을 만든 후, 저장은 단어의 고유 번호(index)로 저장합니다\n",
    "    target = word_dict[word_sequence[i]]\n",
    "    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
    "\n",
    "    # (target, context[0]), (target, context[1])..\n",
    "    for w in context:\n",
    "        skip_grams.append([target, w])\n",
    "\n",
    "\n",
    "# skip-gram 데이터에서 무작위로 데이터를 뽑아 입력값과 출력값의 배치 데이터를 생성하는 함수\n",
    "def random_batch(data, size):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
    "\n",
    "    for i in random_index:\n",
    "        random_inputs.append(data[i][0])  # target\n",
    "        random_labels.append([data[i][1]])  # context word\n",
    "\n",
    "    return random_inputs, random_labels\n",
    "\n",
    "\n",
    "#########\n",
    "# 옵션 설정\n",
    "######\n",
    "# 학습을 반복할 횟수\n",
    "training_epoch = 300\n",
    "# 학습률\n",
    "learning_rate = 0.1\n",
    "# 한 번에 학습할 데이터의 크기\n",
    "batch_size = 20\n",
    "# 단어 벡터를 구성할 임베딩 차원의 크기\n",
    "# 이 예제에서는 x, y 그래프로 표현하기 쉽게 2 개의 값만 출력하도록 합니다.\n",
    "embedding_size = 2\n",
    "# word2vec 모델을 학습시키기 위한 nce_loss 함수에서 사용하기 위한 샘플링 크기\n",
    "# batch_size 보다 작아야 합니다.\n",
    "num_sampled = 15\n",
    "# 총 단어 갯수\n",
    "voc_size = len(word_list)\n",
    "\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "# tf.nn.nce_loss 를 사용하려면 출력값을 이렇게 [batch_size, 1] 구성해야합니다.\n",
    "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "# word2vec 모델의 결과 값인 임베딩 벡터를 저장할 변수입니다.\n",
    "# 총 단어 갯수와 임베딩 갯수를 크기로 하는 두 개의 차원을 갖습니다.\n",
    "embeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "# 임베딩 벡터의 차원에서 학습할 입력값에 대한 행들을 뽑아옵니다.\n",
    "# 예) embeddings     inputs    selected\n",
    "#    [[1, 2, 3]  -> [2, 3] -> [[2, 3, 4]\n",
    "#     [2, 3, 4]                [3, 4, 5]]\n",
    "#     [3, 4, 5]\n",
    "#     [4, 5, 6]]\n",
    "selected_embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "# nce_loss 함수에서 사용할 변수들을 정의합니다.\n",
    "nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "\n",
    "# nce_loss 함수를 직접 구현하려면 매우 복잡하지만,\n",
    "# 함수를 텐서플로우가 제공하므로 그냥 tf.nn.nce_loss 함수를 사용하기만 하면 됩니다.\n",
    "loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(nce_weights, nce_biases, labels, selected_embed, num_sampled, voc_size))\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_epoch + 1):\n",
    "        batch_inputs, batch_labels = random_batch(skip_grams, batch_size)\n",
    "\n",
    "        _, loss_val = sess.run([train_op, loss],\n",
    "                               feed_dict={inputs: batch_inputs,\n",
    "                                          labels: batch_labels})\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(\"loss at step \", step, \": \", loss_val)\n",
    "\n",
    "    # matplot 으로 출력하여 시각적으로 확인해보기 위해\n",
    "    # 임베딩 벡터의 결과 값을 계산하여 저장합니다.\n",
    "    # with 구문 안에서는 sess.run 대신 간단히 eval() 함수를 사용할 수 있습니다.\n",
    "    trained_embeddings = embeddings.eval()\n",
    "\n",
    "\n",
    "#########\n",
    "# 임베딩된 Word2Vec 결과 확인\n",
    "# 결과는 해당 단어들이 얼마나 다른 단어와 인접해 있는지를 보여줍니다.\n",
    "######\n",
    "for i, label in enumerate(word_list):\n",
    "    x, y = trained_embeddings[i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext=(5, 2),\n",
    "                 textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
